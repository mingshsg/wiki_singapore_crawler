# Wikipedia Singapore Crawler / æ–°åŠ å¡ç»´åŸºç™¾ç§‘çˆ¬è™«

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

---

## English

A comprehensive web crawler system that systematically downloads and organizes Wikipedia pages related to Singapore. The crawler starts from the main Singapore category page and recursively processes subcategories and individual articles with advanced error handling and recovery capabilities.

### ğŸš€ Key Features

#### Core Functionality
- **Recursive Crawling**: Processes category pages and follows subcategory links with configurable depth limits
- **Language Filtering**: Intelligently processes English and Chinese content while filtering out other languages
- **Content Processing**: Converts HTML to clean, structured JSON format with metadata
- **Smart Deduplication**: Prevents duplicate downloads and maintains processing state
- **Full Resumability**: Can resume crawling after interruptions without losing progress
- **Organized Storage**: Saves content with clear naming conventions and structured file organization

#### Advanced Error Handling
- **Smart Retry Logic**: Exponential backoff for temporary failures, immediate skip for permanent errors (404, 403, 410, 451)
- **Circuit Breaker Protection**: Prevents infinite retry loops with network connectivity detection
- **Network Resilience**: Tests Google connectivity when retries fail, prompts user for continue/skip decisions
- **Comprehensive Logging**: Detailed error tracking and statistics for audit and debugging
- **Failed URL Recovery**: Dedicated retry script for recovering failed downloads

#### Production Features
- **Thread-Safe Operations**: Proper synchronization for concurrent processing
- **Graceful Shutdown**: Signal handling for clean termination
- **Progress Monitoring**: Real-time status updates and comprehensive statistics
- **State Persistence**: Maintains queue, deduplication, and progress state across sessions
- **Performance Optimization**: Configurable delays and timeout settings for respectful crawling

### ğŸ“¦ Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd wikipedia-singapore-crawler
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Verify installation**
   ```bash
   python main.py --help
   ```

### ğŸ¯ Usage

#### Quick Start
```bash
# Start crawling with default settings
python main.py

# Monitor progress in real-time
python main.py --monitor

# Crawl with custom settings
python main.py --output-dir "./my_data" --max-depth 3 --delay 2.0
```

#### Advanced Usage
```bash
# Full configuration example
python main.py \
    --start-url "https://en.wikipedia.org/wiki/Category:Singapore" \
    --output-dir "./singapore_data" \
    --max-depth 5 \
    --delay 1.5 \
    --max-retries 3 \
    --log-level INFO \
    --monitor
```

#### Command Line Options
- `--start-url`: Starting Wikipedia category URL (default: Singapore category)
- `--output-dir`: Output directory for crawled content (default: ./wiki_data)
- `--max-depth`: Maximum depth for subcategory crawling (default: 5)
- `--delay`: Delay between requests in seconds (default: 1.0)
- `--max-retries`: Maximum retry attempts for failed requests (default: 3)
- `--log-level`: Logging level (DEBUG, INFO, WARNING, ERROR)
- `--config`: Path to configuration file
- `--monitor`: Enable real-time progress monitoring

#### Configuration File
Create a `config.json` file for persistent settings:

```json
{
  "start_url": "https://en.wikipedia.org/wiki/Category:Singapore",
  "output_dir": "./wiki_data",
  "max_depth": 5,
  "request_delay": 1.0,
  "request_timeout": 30,
  "max_retries": 3,
  "supported_languages": ["en", "zh-cn", "zh"],
  "max_filename_length": 200,
  "log_level": "INFO",
  "log_file": "crawler.log"
}
```

Use with: `python main.py --config config.json`

### ğŸ”§ Additional Tools

#### Failed URL Retry Script
Recover any URLs that failed during the initial crawling:

```bash
# Retry all failed URLs
python retry_failed_urls.py

# View demonstration of retry functionality
python demo_retry_failed_urls.py
```

#### Demo Scripts
Explore the crawler's capabilities:

```bash
# Demonstrate error handling
python demo_error_handling.py

# Show connectivity handling and circuit breaker
python demo_connectivity_handling.py
```

#### Testing
Run the comprehensive test suite:

```bash
# Run all tests
python -m pytest tests/

# Run specific test categories
python test_error_handling.py
python test_connectivity_handling.py
python test_retry_functionality.py
```

### ğŸ“ Project Structure

```
wikipedia-singapore-crawler/
â”œâ”€â”€ wikipedia_crawler/          # Main crawler package
â”‚   â”œâ”€â”€ core/                  # Core crawler components
â”‚   â”‚   â”œâ”€â”€ wikipedia_crawler.py    # Main orchestration
â”‚   â”‚   â”œâ”€â”€ page_processor.py       # HTTP requests & error handling
â”‚   â”‚   â”œâ”€â”€ url_queue.py            # URL queue management
â”‚   â”‚   â”œâ”€â”€ deduplication.py        # Duplicate prevention
â”‚   â”‚   â”œâ”€â”€ progress_tracker.py     # Progress monitoring
â”‚   â”‚   â””â”€â”€ file_storage.py         # File operations
â”‚   â”œâ”€â”€ processors/            # Content processing
â”‚   â”‚   â”œâ”€â”€ category_handler.py     # Category page processing
â”‚   â”‚   â”œâ”€â”€ article_handler.py      # Article processing
â”‚   â”‚   â”œâ”€â”€ content_processor.py    # HTML to JSON conversion
â”‚   â”‚   â””â”€â”€ language_filter.py      # Language detection
â”‚   â”œâ”€â”€ models/                # Data models
â”‚   â”‚   â””â”€â”€ data_models.py          # Core data structures
â”‚   â””â”€â”€ utils/                 # Utility functions
â”‚       â”œâ”€â”€ filename_utils.py       # File naming utilities
â”‚       â””â”€â”€ logging_config.py       # Logging setup
â”œâ”€â”€ tests/                     # Comprehensive test suite
â”œâ”€â”€ wiki_data/                 # Output directory (created during crawling)
â”‚   â”œâ”€â”€ state/                 # State persistence files
â”‚   â””â”€â”€ *.json                 # Downloaded Wikipedia content
â”œâ”€â”€ main.py                    # Main entry point
â”œâ”€â”€ retry_failed_urls.py       # Failed URL recovery script
â”œâ”€â”€ demo_*.py                  # Demonstration scripts
â”œâ”€â”€ test_*.py                  # Standalone test scripts
â”œâ”€â”€ config.json                # Configuration file
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

### ğŸ“Š Performance & Statistics

#### Crawling Performance
- **Processing Rate**: ~20 URLs per minute
- **Success Rate**: 99.8% (based on Singapore crawling validation)
- **Error Recovery**: Automatic retry with exponential backoff
- **Memory Efficiency**: Streaming processing with state persistence

#### Monitoring & Reporting
- Real-time progress updates
- Comprehensive error categorization
- Detailed component statistics
- Automatic report generation

### ğŸ› ï¸ Development Status

**Current Version**: Production Ready âœ…

#### Completed Features
- âœ… Core crawling engine with recursive processing
- âœ… Advanced error handling and circuit breaker protection
- âœ… Network connectivity detection and user interaction
- âœ… Failed URL retry system with comprehensive reporting
- âœ… Complete test suite with 100% coverage of critical paths
- âœ… Production-ready logging and monitoring
- âœ… State persistence and resumability
- âœ… Comprehensive documentation

#### Validation Results
- **Total URLs Processed**: 3,097 Singapore-related Wikipedia pages
- **Success Rate**: 99.8% (3,091 successful, 6 failed)
- **Processing Time**: ~2.5 hours for complete Singapore category
- **Data Quality**: All files properly formatted and validated

### ğŸ“‹ Requirements

#### System Requirements
- **Python**: 3.8 or higher
- **Memory**: 512MB RAM minimum (1GB recommended)
- **Storage**: Variable (depends on content volume)
- **Network**: Stable internet connection

#### Python Dependencies
```
requests>=2.31.0      # HTTP requests
beautifulsoup4>=4.12.0 # HTML parsing
markdownify>=0.11.6   # HTML to Markdown conversion
langdetect>=1.0.9     # Language detection
hypothesis>=6.82.0    # Property-based testing
pytest>=7.4.0         # Test framework
```

### ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

### ğŸ“„ License

This project is open source. Please check the license file for details.

---

## ä¸­æ–‡

ä¸€ä¸ªå…¨é¢çš„ç½‘ç»œçˆ¬è™«ç³»ç»Ÿï¼Œç³»ç»Ÿæ€§åœ°ä¸‹è½½å’Œæ•´ç†ä¸æ–°åŠ å¡ç›¸å…³çš„ç»´åŸºç™¾ç§‘é¡µé¢ã€‚çˆ¬è™«ä»æ–°åŠ å¡ä¸»åˆ†ç±»é¡µé¢å¼€å§‹ï¼Œé€’å½’å¤„ç†å­åˆ†ç±»å’Œå•ä¸ªæ–‡ç« ï¼Œå…·å¤‡å…ˆè¿›çš„é”™è¯¯å¤„ç†å’Œæ¢å¤åŠŸèƒ½ã€‚

### ğŸš€ æ ¸å¿ƒç‰¹æ€§

#### åŸºç¡€åŠŸèƒ½
- **é€’å½’çˆ¬å–**: å¤„ç†åˆ†ç±»é¡µé¢å¹¶è·Ÿè¸ªå­åˆ†ç±»é“¾æ¥ï¼Œæ”¯æŒå¯é…ç½®çš„æ·±åº¦é™åˆ¶
- **è¯­è¨€è¿‡æ»¤**: æ™ºèƒ½å¤„ç†è‹±æ–‡å’Œä¸­æ–‡å†…å®¹ï¼Œè¿‡æ»¤å…¶ä»–è¯­è¨€
- **å†…å®¹å¤„ç†**: å°†HTMLè½¬æ¢ä¸ºæ¸…æ´çš„ç»“æ„åŒ–JSONæ ¼å¼ï¼ŒåŒ…å«å…ƒæ•°æ®
- **æ™ºèƒ½å»é‡**: é˜²æ­¢é‡å¤ä¸‹è½½å¹¶ç»´æŠ¤å¤„ç†çŠ¶æ€
- **å®Œå…¨å¯æ¢å¤**: å¯åœ¨ä¸­æ–­åæ¢å¤çˆ¬å–è€Œä¸ä¸¢å¤±è¿›åº¦
- **æœ‰åºå­˜å‚¨**: ä½¿ç”¨æ¸…æ™°çš„å‘½åçº¦å®šå’Œç»“æ„åŒ–æ–‡ä»¶ç»„ç»‡ä¿å­˜å†…å®¹

#### é«˜çº§é”™è¯¯å¤„ç†
- **æ™ºèƒ½é‡è¯•é€»è¾‘**: å¯¹ä¸´æ—¶æ•…éšœä½¿ç”¨æŒ‡æ•°é€€é¿ï¼Œå¯¹æ°¸ä¹…é”™è¯¯(404, 403, 410, 451)ç«‹å³è·³è¿‡
- **æ–­è·¯å™¨ä¿æŠ¤**: é€šè¿‡ç½‘ç»œè¿æ¥æ£€æµ‹é˜²æ­¢æ— é™é‡è¯•å¾ªç¯
- **ç½‘ç»œå¼¹æ€§**: é‡è¯•å¤±è´¥æ—¶æµ‹è¯•Googleè¿æ¥ï¼Œæç¤ºç”¨æˆ·é€‰æ‹©ç»§ç»­/è·³è¿‡
- **å…¨é¢æ—¥å¿—è®°å½•**: è¯¦ç»†çš„é”™è¯¯è·Ÿè¸ªå’Œç»Ÿè®¡ï¼Œç”¨äºå®¡è®¡å’Œè°ƒè¯•
- **å¤±è´¥URLæ¢å¤**: ä¸“ç”¨é‡è¯•è„šæœ¬ç”¨äºæ¢å¤å¤±è´¥çš„ä¸‹è½½

#### ç”Ÿäº§ç‰¹æ€§
- **çº¿ç¨‹å®‰å…¨æ“ä½œ**: ä¸ºå¹¶å‘å¤„ç†æä¾›é€‚å½“çš„åŒæ­¥
- **ä¼˜é›…å…³é—­**: ä¿¡å·å¤„ç†ä»¥å®ç°æ¸…æ´ç»ˆæ­¢
- **è¿›åº¦ç›‘æ§**: å®æ—¶çŠ¶æ€æ›´æ–°å’Œå…¨é¢ç»Ÿè®¡
- **çŠ¶æ€æŒä¹…åŒ–**: è·¨ä¼šè¯ç»´æŠ¤é˜Ÿåˆ—ã€å»é‡å’Œè¿›åº¦çŠ¶æ€
- **æ€§èƒ½ä¼˜åŒ–**: å¯é…ç½®çš„å»¶è¿Ÿå’Œè¶…æ—¶è®¾ç½®ï¼Œå®ç°ç¤¼è²Œçˆ¬å–

### ğŸ“¦ å®‰è£…

1. **å…‹éš†ä»“åº“**
   ```bash
   git clone <repository-url>
   cd wikipedia-singapore-crawler
   ```

2. **å®‰è£…ä¾èµ–**
   ```bash
   pip install -r requirements.txt
   ```

3. **éªŒè¯å®‰è£…**
   ```bash
   python main.py --help
   ```

### ğŸ¯ ä½¿ç”¨æ–¹æ³•

#### å¿«é€Ÿå¼€å§‹
```bash
# ä½¿ç”¨é»˜è®¤è®¾ç½®å¼€å§‹çˆ¬å–
python main.py

# å®æ—¶ç›‘æ§è¿›åº¦
python main.py --monitor

# ä½¿ç”¨è‡ªå®šä¹‰è®¾ç½®çˆ¬å–
python main.py --output-dir "./my_data" --max-depth 3 --delay 2.0
```

#### é«˜çº§ç”¨æ³•
```bash
# å®Œæ•´é…ç½®ç¤ºä¾‹
python main.py \
    --start-url "https://en.wikipedia.org/wiki/Category:Singapore" \
    --output-dir "./singapore_data" \
    --max-depth 5 \
    --delay 1.5 \
    --max-retries 3 \
    --log-level INFO \
    --monitor
```

#### å‘½ä»¤è¡Œé€‰é¡¹
- `--start-url`: èµ·å§‹ç»´åŸºç™¾ç§‘åˆ†ç±»URLï¼ˆé»˜è®¤ï¼šæ–°åŠ å¡åˆ†ç±»ï¼‰
- `--output-dir`: çˆ¬å–å†…å®¹çš„è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼š./wiki_dataï¼‰
- `--max-depth`: å­åˆ†ç±»çˆ¬å–çš„æœ€å¤§æ·±åº¦ï¼ˆé»˜è®¤ï¼š5ï¼‰
- `--delay`: è¯·æ±‚é—´å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤ï¼š1.0ï¼‰
- `--max-retries`: å¤±è´¥è¯·æ±‚çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ï¼š3ï¼‰
- `--log-level`: æ—¥å¿—çº§åˆ«ï¼ˆDEBUG, INFO, WARNING, ERRORï¼‰
- `--config`: é…ç½®æ–‡ä»¶è·¯å¾„
- `--monitor`: å¯ç”¨å®æ—¶è¿›åº¦ç›‘æ§

#### é…ç½®æ–‡ä»¶
åˆ›å»º `config.json` æ–‡ä»¶ç”¨äºæŒä¹…åŒ–è®¾ç½®ï¼š

```json
{
  "start_url": "https://en.wikipedia.org/wiki/Category:Singapore",
  "output_dir": "./wiki_data",
  "max_depth": 5,
  "request_delay": 1.0,
  "request_timeout": 30,
  "max_retries": 3,
  "supported_languages": ["en", "zh-cn", "zh"],
  "max_filename_length": 200,
  "log_level": "INFO",
  "log_file": "crawler.log"
}
```

ä½¿ç”¨æ–¹æ³•ï¼š`python main.py --config config.json`

### ğŸ”§ é™„åŠ å·¥å…·

#### å¤±è´¥URLé‡è¯•è„šæœ¬
æ¢å¤åˆå§‹çˆ¬å–è¿‡ç¨‹ä¸­å¤±è´¥çš„ä»»ä½•URLï¼š

```bash
# é‡è¯•æ‰€æœ‰å¤±è´¥çš„URL
python retry_failed_urls.py

# æŸ¥çœ‹é‡è¯•åŠŸèƒ½æ¼”ç¤º
python demo_retry_failed_urls.py
```

#### æ¼”ç¤ºè„šæœ¬
æ¢ç´¢çˆ¬è™«çš„åŠŸèƒ½ï¼š

```bash
# æ¼”ç¤ºé”™è¯¯å¤„ç†
python demo_error_handling.py

# æ˜¾ç¤ºè¿æ¥å¤„ç†å’Œæ–­è·¯å™¨
python demo_connectivity_handling.py
```

#### æµ‹è¯•
è¿è¡Œå…¨é¢çš„æµ‹è¯•å¥—ä»¶ï¼š

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python -m pytest tests/

# è¿è¡Œç‰¹å®šæµ‹è¯•ç±»åˆ«
python test_error_handling.py
python test_connectivity_handling.py
python test_retry_functionality.py
```

### ğŸ“ é¡¹ç›®ç»“æ„

```
wikipedia-singapore-crawler/
â”œâ”€â”€ wikipedia_crawler/          # ä¸»çˆ¬è™«åŒ…
â”‚   â”œâ”€â”€ core/                  # æ ¸å¿ƒçˆ¬è™«ç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ wikipedia_crawler.py    # ä¸»è¦ç¼–æ’
â”‚   â”‚   â”œâ”€â”€ page_processor.py       # HTTPè¯·æ±‚å’Œé”™è¯¯å¤„ç†
â”‚   â”‚   â”œâ”€â”€ url_queue.py            # URLé˜Ÿåˆ—ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ deduplication.py        # é‡å¤é˜²æ­¢
â”‚   â”‚   â”œâ”€â”€ progress_tracker.py     # è¿›åº¦ç›‘æ§
â”‚   â”‚   â””â”€â”€ file_storage.py         # æ–‡ä»¶æ“ä½œ
â”‚   â”œâ”€â”€ processors/            # å†…å®¹å¤„ç†
â”‚   â”‚   â”œâ”€â”€ category_handler.py     # åˆ†ç±»é¡µé¢å¤„ç†
â”‚   â”‚   â”œâ”€â”€ article_handler.py      # æ–‡ç« å¤„ç†
â”‚   â”‚   â”œâ”€â”€ content_processor.py    # HTMLåˆ°JSONè½¬æ¢
â”‚   â”‚   â””â”€â”€ language_filter.py      # è¯­è¨€æ£€æµ‹
â”‚   â”œâ”€â”€ models/                # æ•°æ®æ¨¡å‹
â”‚   â”‚   â””â”€â”€ data_models.py          # æ ¸å¿ƒæ•°æ®ç»“æ„
â”‚   â””â”€â”€ utils/                 # å®ç”¨å·¥å…·
â”‚       â”œâ”€â”€ filename_utils.py       # æ–‡ä»¶å‘½åå·¥å…·
â”‚       â””â”€â”€ logging_config.py       # æ—¥å¿—è®¾ç½®
â”œâ”€â”€ tests/                     # å…¨é¢æµ‹è¯•å¥—ä»¶
â”œâ”€â”€ wiki_data/                 # è¾“å‡ºç›®å½•ï¼ˆçˆ¬å–æ—¶åˆ›å»ºï¼‰
â”‚   â”œâ”€â”€ state/                 # çŠ¶æ€æŒä¹…åŒ–æ–‡ä»¶
â”‚   â””â”€â”€ *.json                 # ä¸‹è½½çš„ç»´åŸºç™¾ç§‘å†…å®¹
â”œâ”€â”€ main.py                    # ä¸»å…¥å£ç‚¹
â”œâ”€â”€ retry_failed_urls.py       # å¤±è´¥URLæ¢å¤è„šæœ¬
â”œâ”€â”€ demo_*.py                  # æ¼”ç¤ºè„šæœ¬
â”œâ”€â”€ test_*.py                  # ç‹¬ç«‹æµ‹è¯•è„šæœ¬
â”œâ”€â”€ config.json                # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt           # Pythonä¾èµ–
â””â”€â”€ README.md                  # æœ¬æ–‡ä»¶
```

### ğŸ“Š æ€§èƒ½ä¸ç»Ÿè®¡

#### çˆ¬å–æ€§èƒ½
- **å¤„ç†é€Ÿåº¦**: çº¦æ¯åˆ†é’Ÿ20ä¸ªURL
- **æˆåŠŸç‡**: 99.8%ï¼ˆåŸºäºæ–°åŠ å¡çˆ¬å–éªŒè¯ï¼‰
- **é”™è¯¯æ¢å¤**: è‡ªåŠ¨é‡è¯•ï¼ŒæŒ‡æ•°é€€é¿
- **å†…å­˜æ•ˆç‡**: æµå¼å¤„ç†ï¼ŒçŠ¶æ€æŒä¹…åŒ–

#### ç›‘æ§ä¸æŠ¥å‘Š
- å®æ—¶è¿›åº¦æ›´æ–°
- å…¨é¢é”™è¯¯åˆ†ç±»
- è¯¦ç»†ç»„ä»¶ç»Ÿè®¡
- è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆ

### ğŸ› ï¸ å¼€å‘çŠ¶æ€

**å½“å‰ç‰ˆæœ¬**: ç”Ÿäº§å°±ç»ª âœ…

#### å·²å®ŒæˆåŠŸèƒ½
- âœ… å…·æœ‰é€’å½’å¤„ç†çš„æ ¸å¿ƒçˆ¬å–å¼•æ“
- âœ… é«˜çº§é”™è¯¯å¤„ç†å’Œæ–­è·¯å™¨ä¿æŠ¤
- âœ… ç½‘ç»œè¿æ¥æ£€æµ‹å’Œç”¨æˆ·äº¤äº’
- âœ… å¤±è´¥URLé‡è¯•ç³»ç»Ÿï¼ŒåŒ…å«å…¨é¢æŠ¥å‘Š
- âœ… å®Œæ•´æµ‹è¯•å¥—ä»¶ï¼Œå…³é”®è·¯å¾„100%è¦†ç›–
- âœ… ç”Ÿäº§å°±ç»ªçš„æ—¥å¿—è®°å½•å’Œç›‘æ§
- âœ… çŠ¶æ€æŒä¹…åŒ–å’Œå¯æ¢å¤æ€§
- âœ… å…¨é¢æ–‡æ¡£

#### éªŒè¯ç»“æœ
- **å¤„ç†çš„æ€»URLæ•°**: 3,097ä¸ªæ–°åŠ å¡ç›¸å…³ç»´åŸºç™¾ç§‘é¡µé¢
- **æˆåŠŸç‡**: 99.8%ï¼ˆ3,091ä¸ªæˆåŠŸï¼Œ6ä¸ªå¤±è´¥ï¼‰
- **å¤„ç†æ—¶é—´**: å®Œæ•´æ–°åŠ å¡åˆ†ç±»çº¦2.5å°æ—¶
- **æ•°æ®è´¨é‡**: æ‰€æœ‰æ–‡ä»¶æ ¼å¼æ­£ç¡®å¹¶å·²éªŒè¯

### ğŸ“‹ ç³»ç»Ÿè¦æ±‚

#### ç³»ç»Ÿéœ€æ±‚
- **Python**: 3.8æˆ–æ›´é«˜ç‰ˆæœ¬
- **å†…å­˜**: æœ€å°‘512MB RAMï¼ˆæ¨è1GBï¼‰
- **å­˜å‚¨**: å¯å˜ï¼ˆå–å†³äºå†…å®¹é‡ï¼‰
- **ç½‘ç»œ**: ç¨³å®šçš„äº’è”ç½‘è¿æ¥

#### Pythonä¾èµ–
```
requests>=2.31.0      # HTTPè¯·æ±‚
beautifulsoup4>=4.12.0 # HTMLè§£æ
markdownify>=0.11.6   # HTMLåˆ°Markdownè½¬æ¢
langdetect>=1.0.9     # è¯­è¨€æ£€æµ‹
hypothesis>=6.82.0    # åŸºäºå±æ€§çš„æµ‹è¯•
pytest>=7.4.0         # æµ‹è¯•æ¡†æ¶
```

### ğŸ¤ è´¡çŒ®

1. Forkä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. ä¸ºæ–°åŠŸèƒ½æ·»åŠ æµ‹è¯•
4. ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡
5. æäº¤æ‹‰å–è¯·æ±‚

### ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä¸ºå¼€æºé¡¹ç›®ã€‚è¯·æŸ¥çœ‹è®¸å¯è¯æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚